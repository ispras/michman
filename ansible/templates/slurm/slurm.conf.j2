#Configuration file for slurm
#Authentification
AuthType=auth/munge
#JWT autentification for slurmrestd
{%if slurm_use_rest %} 
AuthAltTypes=auth/jwt
AuthAltParameters=jwt_key=/etc/slurm/jwt_hs256.key
{% endif %}

#The accounting storage mechanism type.
{%if slurm_use_db%}
AccountingStorageType= accounting_storage/slurmdbd
{% else %}
AccountingStorageType= accounting_storage/none
{% endif %}


#The name of the machine hosting the accounting storage database.
AccountingStorageHost= {{ACCOUNTING_STORAGE_HOST}}

#The name by which this Slurm managed cluster is known in the accounting database. 
ClusterName= {{CLUSTER_NAME}}

#Hostname of the machine where Slurm control daemon is executed
{%if OS_IMAGE%}
SlurmctldHost = {{SLURM_MASTER}}
{% else %}
ControlMachine= {{SLURM_MASTER}}
{% endif %}

#The job accounting mechanism type. 
JobAcctGatherType= jobacct_gather/linux

#If set to "ALL" then jobs which exceed a partition's size and/or time limits will be rejected at submission time. 
#If job is submitted to multiple partitions, the job must satisfy the limits on all the requested partitions.
EnforcePartLimits= ALL

#Identifies the plugin to be used for process tracking on a job step basis. 
#proctrack/cgroup. Uses linux cgroups to constrain and track processes, and is the default for systems with cgroup support. 
ProctrackType= proctrack/cgroup

#Controls when a DOWN node will be returned to service. 
# 1  means,  DOWN node will become available for use upon registration with a valid configuration only if it was set DOWN due to being non-responsive. 
#For any other reason, its state will not automatically be changed. 
ReturnToService= 1

#Identifies the type of scheduler to be used.
#For a backfill scheduling module to augment the default FIFO scheduling. 
#Backfill scheduling will initiate lower-priority jobs if doing so does not delay the expected initiation time of any higher priority job.
SchedulerType= sched/backfill

#Identifies the type of resource selection algorithm to be used.
#select/cons_res - the resources (cores and memory) within a node are individually allocated as consumable resources.
#SelectType= select/cons_res

#Parameters for SelectType
#SelectTypeParameters= CR_CPU_Memory

#Maximum real memory size available per allocated CPU in megabytes. Used to avoid over-subscribing memory and causing paging.
#MaxMemPerCPU=900

#Default real memory size available per allocated CPU in megabytes. Parameter iss used, when SelectType = select/cons_res or select/cons_tres 
#and consumable resources include memory and CPUs/cores
#DefMemPerCPU=900

#Pathname of a file into which the slurmctld daemon's logs are written.
SlurmctldLogFile= {{SLURM_PATH_TO_LOG_FILE}}/slurmctld.log

#Pathname of a file into which the slurmctld daemon may write its process id. 
SlurmctldPidFile= {{SLURM_PATH_TO_PID_FILE}}/slurmctld.pid

#Pathname of a file into which the slurmd daemon's logs are written. 
SlurmdLogFile= {{SLURM_PATH_TO_LOG_FILE}}/slurmd.log

#Pathname of a file into which the slurmd daemon may write its process id. 
SlurmdPidFile= {{SLURM_PATH_TO_PID_FILE}}/slurmd.pid

#Pathname of a directory into which the slurmd daemon's state information 
#and batch job script information are written.
SlurmdSpoolDir= {{STATE_SAVE_LOCATION_SLAVE}}

#Pathname of a directory into which the Slurm controller, slurmctld, saves its state.
#Slurm state will be saved here to recover from system failures.
StateSaveLocation= {{STATE_SAVE_LOCATION_MASTER}}

#Identifies the type of task launch plugin, typically used to provide resource management within a node.
#task/affinity enables resource containment using sched_setaffinity(). 
#task/cgroup enables resource containment using Linux control cgroups.
TaskPlugin= task/affinity,task/cgroup

#Optional parameters for the task plugin. 
TaskPluginParam= {{Task_Plugin_Param}} 

#Nodes 
#CPUs - number of logical processors on the node.
#RealMemory - size of real memory on the node in megabytes. Parameter is going to be set, if memory is consumable resource.
    NodeName = {{ cluster_name }}-slave-[1-{{n_slaves}}] #CPUs=8 RealMemory=7951

 #Partition
   {% set amount_of_slaves = namespace( value=1)  %}
   {%  for partition in slurm_partitions %} 
   {%    set list = partition.split(":") %}  
   {% set number = list[1] | int  %}
    PartitionName={{ list[0] }} Nodes  = {% if list[1] == "all" %} {{ cluster_name }}-slave-[1-{{n_slaves}}] {% else  %} {{ cluster_name }}-slave-[{{amount_of_slaves.value}}-{{ number + amount_of_slaves.value - 1}}] {% endif %} Default= {% if list[0] == "main" %} YES {% else %} NO {% endif %} 
   {% set  amount_of_slaves.value = number + amount_of_slaves.value %}
  {% endfor %}
